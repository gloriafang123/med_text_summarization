1. Introduction

Breast cancer is the most common form of cancer in the female population. It is estimated that approximately 12%of women in the USA will be diagnosed with breast cancer at some point during their lifetime[1]. Breast cancer has the highest incidence and mortality rate amongst all cancers (excluding melanoma skin cancer)[2]. In the EU, breast cancer is the leading cause of mortality amongst the female population, accounting for 15.6% in 2015[3].

Although breast cancer incidence has increased in the past decade, the introduction of screening programmes for early detection has achieved a lowering of the mortality rate. The conventional imaging modality used for screening is X-ray mammography, being both a fast and cost-effective technique for screening a large population. In this technique, images of each breast are typically acquired using two different views: cranio-caudal (CC) imaged from top to bottom and medio-lateral oblique (MLO) from left to right. With advancements in imaging techniques, high quality full-field digital mammograms (FFDM) have replaced the traditional scanned-film mammograms. Furthermore, developments in computer technology and data science has generated interest in exploring deep learning methods for various tasks including object detection[4], [5] and image recognition[6], [7].

Deep learning methods based on convolutional neural networks (CNN) have also gained importance in the field of medical image analysis and efforts have been made to develop modern computer-aided detection (CAD) systems based on these CNN algorithms[8], [9], [10]. Additionally, in mammography, some authors have also proposed the use of traditional machine learning based on handcrafted feature to classify masses[11], [12], [13], [14]. However, the exploration of deep learning methods in the field of breast imaging has been limited, as only a small number of public datasets are available (e.g.DDSM[15], INbreast[16]).

Although researchers have used Faster R-CNN in medical imaging[17], [18], there is a dearth of literature in the breast imaging field. For instance, Akselrod-Ballin etal.[19] used a modified version of a Faster R-CNN model to include information from the finer bottom levels during the classification stage. Ribli etal.[20] trained a Faster R-CNN model on the DDSM database composed of 2620 scanned-film mammograms and then evaluated the performance of the network in the INbreast dataset of malignant masses.

Jung etal.[21] proposed a mass detection model based on RetinaNet[22] using a new loss function, called focal loss, to address the problem of extreme class imbalance between the foreground and background. The performance of the network was evaluated on a combination of a private (GURO) and public (INbreast) dataset. Morrel etal.[23] presented a neural network based on a region-based fully convolutional network (R-FCN)[24] and deformable convolutional nets. Although the network was trained using the OPTIMAM Mammography Image Database (OMI-DB)[25], the results were only provided for the DREAMS challenge[26] competitive phase.

Recently, Al-masni etal.[9] and Al-antari etal.[10] adopted the You Only Look Once (YOLO) deep learning method[27] for the detection and classification of masses in mammograms. One of the major advantages of the YOLO algorithm is speed, as it defines the object detection as a regression problem. However, YOLO is limited in terms of accuracy and precision in the localisation of small objects[27].

In our previous work[28], an automated mass detection framework using CNN was presented. Here small regions of the mammograms (patches) were extracted using a sliding window approach and used for training different CNNs. The framework obtained results comparable to state-of-the-art on the INbreast dataset. However, the high computational cost was a limiting factor for clinical use.

In this paper, a mass detection framework based on Faster R-CNN object detection model is presented. This uses the whole FFDM (instead of patch-based strategy) for training and testing and is based on the “recognition using regions” paradigm[29]. The proposed framework is evaluated on the large mammography OMI-DB[25] dataset containing images from 4750 cases. The key contributions of this paper are summarised below:

1.

    The implementation of Faster R-CNN model for detecting masses in a large-scale mammography dataset of malignant masses (OMI-DB).
2.

    To the best of our knowledge, this paper is the first to benchmark the performance of a deep learning method on the OMI-DB dataset.
3.

    The application of transfer learning to detect masses in two small mammography datasets obtained using different scanners.
4.

    Comparison with other works in the literature, showing that the proposed mass detection framework achieves better performance in terms of higher True Positive Rate (TPR) with lower False Positives per Image (FPI).

The remainder of this paper is structured as follows: Section2 details the datasets used; Section3 describes the Faster R-CNN model; Section4 describes the methodology for training and testing the Faster R-CNN model, and Section5 details the experimental results. Discussions are presented in Section6, and in Section7 we provide conclusions and suggested future work.
2. Datasets
2.1. OPTIMAM mammography database (OMI-DB)

The OMI-DB[25] is an extensive mammography image database of over 145,000 cases (over 2.4 million images) comprised of unprocessed and processed FFDMs from the UK’s National Health Service Breast Screening Program. It also contains expert’s determined ground truths and associated clinical data linked to the images. As part of the data sharing agreement with the Royal Surrey County Hospital (UK) in 2017, we obtained a subset of this database (4750 cases with 

80,000 processed and unprocessed FFDMs). The database contains images from different manufacturers, particularly Hologic Inc, Marlborough, Massachusetts, USA (Hologic Lorad Selenia and Selenia Dimensions Mammography Systems), and General Electric (GE) Medical Systems, Chicago, Illinois, USA (Senograph DS and Senographe Essential), referred to as OMI-H and OMI-G, respectively. For each case, two views of each breast, i.e.medio-lateral oblique (MLO) and cranio-caudal (CC) are available, together with several other views[30] for cases with suspected abnormalities. In this work, only the processed FFDMs with expert’s annotated ground-truth are used, resulting in a total of 2145 cases with cancers.

There are several breast abnormalities in the OMI-DB dataset, such as masses, calcifications, architectural distortions, focal asymmetries, or combinations of the above. Since the focus of this paper is on the detection of masses or mass like abnormalities, mammograms with calcifications only are not considered, while architectural distortions and focal asymmetries are included in the dataset. The categorisation of the OMI-DB dataset based on BI-RADS[31] ratings and mass conspicuity is shown in Fig.1 (only the biopsy proven BI-RADS ratings are considered). The OMI-H and OMI-G dataset contained, respectively 2042 and 103 positive cases, with abnormalities in either one of the mammography views (CC and MLO), and 842 and 104 normal cases, i.e.without any abnormalities.

    Download : Download high-res image (105KB)Download : Download full-size image

Fig. 1. Categorisation of the OMI-DB dataset using (a) BI-RADS ratings, and (b) mass conspicuity.
2.2. INbreast

The INbreast[16] public dataset is composed of FFDMs acquired using Siemens MammoNovation mammography system (Siemens Healthineers, Erlangen, Germany). The FFDMs are acquired from 115 cases with CC and MLO mammography views, leading to a total of 410 FFDMs available in DICOM format. From these, a total of 116 masses can be found in 107 mammograms from 50 cases. The masses are divided into benign (BI-RADS 
{2,3}) and malignant (BI-RADS 

{4,5,6}).
2.3. Data preparation

Details of the dataset in this work is shown in Table1, where Cases refers to patients with different mammographic views and Images is the number of processed images. The positive images are obtained from the cases with abnormalities, while all the negative images are obtained from those with no abnormalities (normal cases). The extra negative images of the cases with abnormalities, in which there is only one breast or one view, are not considered as they are the clinical history of the same patient.

The OMI-H dataset is divided into training, validation, and testing sets in the ratio of 70%, 10%, and 20% respectively. In the OMI-G and INbreast datasets, a 5-fold cross-validation strategy is used to test all the mammograms in the datasets. The division of images is done on a patient basis such that all the mammograms from an individual case belongs exclusively in either training or testing set. Note that some images in the OMI-H and OMI-G datasets (referred to as unknown), whose BI-RADS ratings are not clearly stated, are also annotated as masses by the radiologists. Therefore, we used these cases for training the model using the label as mass.

Table 1. Description of dataset used in this work: Pos refers to positives (masses) and Neg refers to negatives (non-mass).
Dataset	OMI-H	OMI-G	INbreast
Resolution 
	70, 60	100	70
Image sizes (pixels)	3328×2560	2294×1914	3328×2560
4096×3328	3062×2394	3328×4084
Cases	Pos	2042	103	50
Neg	842	104	65
Images	Pos	3770	195	107
Neg	3475	406	303
Mass	Benign	485	11	40
Malign	3048	143	75
Unknown	684	52	n/a
Image distribution	Training	5316	5-folds cross-validation	5-folds cross-validation
Validation	585
Testing	1344
2.4. Data pre-processing

The original mammograms in the datasets are high-resolution images (
60–) with sizes in the range of 2000-4000 pixels. To focus the processing in the areas with intensity information (non-background pixels), mammograms are cropped identifying the breast area bounding box. Subsequently, and in line with other works in the literature, mammograms are down-sampled to 

for computational and memory limitations. Moreover, the images in the OMI-G and INbreast datasets have very different contrast compared to those in the OMI-H dataset, so image normalisation is performed on the images in these datasets. In the OMI-G, a window width (WW) and window center (WC) normalisation are applied[32]. The WW and WC information is obtained from the DICOM header of the mammograms. For images in the INbreast dataset, an adjustment is made on the windows of the pixel levels. The images are normalised (similar to Ribli etal.[20]) and intensity re-scaled to 8 bits.
3. Faster R-CNN

The Faster R-CNN model has been widely used for detecting objects particularly in natural image datasets, e.g.PASCAL VOC[34], MS-COCO[35], etc. Advancements in the field of object detection are often built on the success of region proposal methods[36] and R-CNNs[4]. For example, in 2015, Ren etal.[5] introduced region proposal network (RPN), which takes an image as the input and outputs a set of rectangular boxes used to detect and localise objects in the image. These rectangular boxes are characterised by an “objectness” score, which is a measure of closeness of the detected object to a certain object class. The Faster R-CNN[5] uses RPN, along with the Fast R-CNN[37] model, to accelerate the training and testing processes, and to improve performance. In the Faster R-CNN paradigm, the problem of object detection is considered as both a regression and classification problem. Additionally, during training of the Faster-RCNN model, a class balance is performed using a mini-batch obtained from a single image, such that positive and negative anchors are obtained in a 1:1 ratio, and the loss function of the mini-batch is then computed[5].

Table 2. Some of the pre-trained Faster R-CNN models available in[33].
Model	Training dataset	Speed
(ms)	mean average precision
Faster_RCNN_Inception_v2	MS COCO	58	28
Faster_RCNN_Resnet50	MS COCO	89	30
Faster_RCNN_Resnet101	MS COCO	106	32
Faster_RCNN_Inception_Resnet_v2_atrous_v2	Open Images	727	37
Faster_RCNN_Resnet101_fgvc	iNaturalist	395	58

In the Faster R-CNN API[33], a collection of pre-trained detection models is provided, which includes: 21 models pre-trained on MS-COCO dataset[35], 1 model on KITTI dataset[38], 6 models on Open Images[39], and 2 models on iNaturalist species[40]. Table2 presents some of the available pre-trained models along with their speed. In this paper, the Faster R-CNN model (InceptionV2[41] as feature extractor) pre-trained on the MS-COCO dataset is used. This selection is based on the performance of Faster R-CNN with different backbone models and datasets in terms of trade-off between speed and average precision.

    Download : Download high-res image (179KB)Download : Download full-size image

Fig. 2. Flow chart of the Faster R-CNN, showing the region proposal network (RPN) and the overall pipeline. The convolutional layer of the InceptionV2 is used as feature extractor, which shares the feature map with the RPN, generating the region proposals. The classification and regression problem are solved on these proposals to generate the final bounding box information.
4. Methodology

In the first step, FFDM is used as the input to the Faster R-CNN model. This is forwarded through the InceptionV2[41] based CNN model to produce the feature map. In the second step, an RPN is created using the extracted features of the CNN and is trained to detect and localise masses on the mammograms. A window of size 

slides over the feature map and outputs a feature vector linked to two fully convolutional (FC) layers, i.e.box-regression layer (reg) and box-classification layer (cls). Thereafter, the anchors or bounding boxes are created to generate region proposals of varying shapes and sizes, and are given an objectness score signifying how accurately they are enclosing a mass on the mammogram. The highest scoring anchors are then passed to the second stage of the network. Here, a classification and regression problem is solved to accurately detect the presence of masses, and simultaneously refine the coordinates of the anchors to precisely detect them. In the last step, the best predictions are obtained by using non-maximum suppression on the detected overlapping objects, resulting in the final detected bounding box with confidence probability representing how close it represents a mass (Fig.2).

In this paper, the Faster R-CNN model proposed by Renetal.[5] is adapted to generate region proposals for varying shapes and sizes, and are labelled as positives (representing masses) and negatives (representing background or non-mass region) for all the mammograms. Note that all the computations are performed on a Linux workstation with 12 CPU cores (3.4GHz) and an NVIDIA TitanX Pascal GPU with 12GB memory.
4.1. Training and hyperparameter tuning

The Faster R-CNN model is implemented within the Tensorflow object detection API[33]. The input to the model is in the form of Tfrecords (containing the mammograms along with class definitions, bounding box coordinates, etc.). The training is performed using the hyperparameter “keep aspect ratio” with the maximum height and width of the mammograms in the entire dataset. As we have a large dataset, only horizontal flipping is applied as data augmentation during the training process. In terms of the optimiser, the Stochastic gradient descent method with momentum[42] is used, with a momentum value of 0.9. During training, the learning rate is heuristically decreased in steps after every 25,000 iterations and continued until 200,000 iterations. Additionally, as all the mammograms in the dataset are cropped to the breast profile resulting in the mammograms with different pixel dimensions, a batch size of 1 is used for training.

The anchors are created with a base size of 128 pixels, three aspect ratios (0.5, 1.0, and 2.0), and five different scales (0.1, 0.2, 0.5, 1.0, and 2.0), resulting in a total of 15 anchors at a defined pixel location. This selection of anchor scales and aspect ratios is done based on the average size distribution of the ground-truth masses in the entire dataset. At the second stage, the detections are processed with non-maximum suppression using a threshold of 0.05, resulting in non-overlapping bounding box predictions to represent masses in the mammogram. This is done to avoid overlapping detection boxes, as in mammograms it is less likely to have overlapping masses[20].
4.2. Transfer learning

Transfer learning (also known as domain adaptation) is considered to be an efficient methodology, in which the knowledge from one image domain can be transferred to another image domain[43]. Azizpour etal.[44] suggests that the success of any transfer learning approach highly depends on the extent of similarity between the databases on which a CNN is pre-trained and the database to which the image features are being transferred. In this paper, the transfer learning methodology is used to fine-tune the Faster R-CNN model pre-trained on a large mammography dataset (OMI-H) to detect masses in small mammography datasets (OMI-G and INbreast) obtained using different scanners.

    Download : Download high-res image (96KB)Download : Download full-size image

Fig. 3. Comparison of TPR vs IoU for the detection of masses in OMI-H testing dataset.
4.3. Evaluation metric

In this work, the objectness score is obtained as an output of the network, which is then used as the confidence probability to construct the confusion matrix. To assess the classification and detection performance of the proposed framework, only the bounding boxes with confidence probability greater than a particular threshold are considered. Herein, the confidence threshold is varied between 0.01–0.99 to plot the free-receiver operating curve (FROC). The qualitative assessment is made using the confusion matrix to compute the sensitivity, specificity, and F1-score of the proposed framework as:  (1)
where, TP, TN, FP, and FN are true positives, true negatives, false positives, and false negatives per mammograms, respectively. For the detection framework, a mass is considered to be detected as a true positive (TP), if the intersection over union (IoU), defining the overlapping area between the predicted box, and the ground truth (GT) box, is greater than a pre-defined threshold. This threshold is obtained by evaluating the detection performance at different IoUs (Fig.3), where the TPR is plotted along with the detection IoU. It can be seen that TPR is almost constant for 0.0  IoU  0.1, slightly reduces (0.930.88) for 0.1  IoU  0.3, and starts to fall sharply for IoU 

0.6. Therefore, in further sections, an IoU of 0.1 is used for all testing sets to compare the predicted and GT results (as used by Ribli etal.[20]).

In cases of multiple masses in a single mammogram, the confusion matrix is computed for the whole mammogram. The IoU is calculated separately for each mass, and if IoU 
0.1 for one of the predicted bounding boxes, the masses are considered to be detected (true positive, TP). Masses that remained undetected are considered as false negatives (FNs), while all remaining prediction boxes with IoU  0.1 are considered false positives (FPs) for the mammogram. The area (

) under the receiver operative curve (ROC) is also used to evaluate the mass classification results. The accuracy of the detection results is assessed using the FROC curve, which is plotted as the function of True Positive Rate (TPR) versus the False Positives per Image (FPI).
5. Experimental results
5.1. Mass detection on OMI-H

In this section, a domain transfer is performed between natural images and mammograms to detect masses in the latter. This is done by adapting the Faster R-CNN model, pre-trained on a large dataset of natural images (MS-COCO), to detect masses in the mammograms. The pre-trained Faster R-CNN model is fine-tuned using a training set of 5316 processed mammograms acquired using the Hologic scanner (OMI-H). Note that the training for lesion and non-lesion is done using a balanced dataset (Table1). The model obtained after training is tested on 1344 (testing set) mammograms, and the predictions on each mammogram compared against the available ground truth (GT) bounding box annotations.

The model’s performance is evaluated using the ROC curve (Fig.4(a)) achieving 
and  on the training and testing data respectively. Additionally, in the plotted FROC curve (Fig.4(b)), it can be seen that the best result of TPR  0.91 at 0.82 FPI is obtained on the training data, and TPR 

0.87 at 0.84 FPI is obtained on the testing data. Furthermore, F1-score of 0.734 has been obtained for the testing data.

In the field of breast cancer imaging, the limited availability of large annotated datasets has been a limiting factor for the success of deep learning methodologies. In the following sections, the transfer learning methodology is used to fine-tune the Faster R-CNN model pre-trained on the OMI-H dataset to detect masses in small mammography datasets: OMI-G and INbreast.

    Download : Download high-res image (265KB)Download : Download full-size image

Fig. 4. Mass detection on OMI-H training and testing dataset, (a) ROC curve, (b) FROC curve.

    Download : Download high-res image (311KB)Download : Download full-size image

Fig. 5. Mass detection on OMI-G testing dataset, (a) ROC curve, (b) FROC curve. The results are shown for OMI-H model tested directly on OMI-G and OMI-H model fine-tuned on OMI-G.
5.2. Mass detection in OMI-G

To train the Faster-RCNN model using a small mammography dataset OMI-G, an analysis is performed. To achieve this, the OMI-G dataset is divided, based on individual cases, into the training (60%), validation (20%) and test sets (20%), and a 5-fold cross-validation strategy is used to test all the mammograms in the OMI-G dataset. The trained model showed reduced performance achieving an 
and TPR of 0.60 at 0.20 FPI. Thereafter, the model trained on the OMI-H dataset is used (without fine-tuning) to detect and localise masses in the test set of the OMI-G dataset, resulting in TPR of 0.70 at 0.43 FPI and 

.

Lastly, fine-tuning is used to adapt the feature domain of the Faster R-CNN model, trained on OMI-H dataset, to detect masses in the OMI-G dataset using a 5-fold cross-validation strategy, resulting in TPR of 0.91±0.06 at 1.70 FPI, 

, and F1-score of 0.80. The per-fold confusion matrix results are presented in Table3, while the overall detection results are summarised in Table4. Additionally, the ROC and FROC curves from these experiments are shown in Fig. 5, Fig. 5 respectively.

Table 3. Per-fold confusion matrix results: OMI-H
OMI-G (confidence probability 

0.20).
	TP	FP	FN	TN
Fold 1	22	22	5	67
Fold 2	18	24	2	71
Fold 3	20	22	5	63
Fold 4	17	12	8	80
Fold 5	12	9	3	71

Table 4. Mass detection results: OMI-H 

OMI-G.
Model	Trained on OMI-G	Trained on OMI-H	Fine tuned on OMI-G
Sensitivity	0.58	0.57	0.76
Specificity	0.88	0.88	0.88
	0.76	0.77	0.87
TPR at FPI	0.59±0.04 at 0.13	0.63 at 0.20	0.74±0.06 at 0.20
0.70 at 0.43	0.91±0.06 at 1.69
5.3. Mass detection in INbreast

The Faster R-CNN model is also used to detect masses in the INbreast public dataset. While the proposed detection framework is used to detect masses in general, the performance is also analysed based on malignant and benign lesions.

Firstly, the performance of the Faster R-CNN model, pre-trained on the OMI-H dataset is analysed to directly detect the masses in the full INbreast dataset (without fine-tuning), resulting in 
for the malignant masses (BI-RADS {4,5,6}), and  for the benign masses (BI-RADS 

{2,3}). Moreover, the FROC analysis resulted in a TPR of 0.87 at 0.32 FPI for the malignant masses, and 0.55 at 0.32 FPI for the benign masses.

Secondly, the model, trained on the OMI-H dataset, is fine-tuned on the INbreast dataset. A 5-fold cross-validation strategy is used to analyse the entire INbreast dataset. In contrast to OMI-DB, there are fewer positive cases (50 numbers) in the INbreast dataset. Therefore, an augmented training dataset is created using very small rotation angles, i.e.

. This data augmentation is used only for the purpose of training the model.

The detection performance is analysed using the ROC and FROC curves as shown in Fig.6(a), and Fig.6(b) respectively. The detection framework of the fine-tuned model (OMI-H 
INbreast) resulted in a sensitivity of 0.990.03 at 1.17 FPI () with F1-score of 0.86 for the malignant masses, and 0.850.08 at 1.0 FPI (

) with F1-score of 0.74 for the benign masses. The per-fold confusion matrix results are presented in Table5, while a summary of the overall detection results is presented in Table6.

    Download : Download high-res image (309KB)Download : Download full-size image

Fig. 6. Mass detection on INbreast testing dataset, (a) ROC curve (b) FROC curve. The results are shown for malignant, benign and all masses.

    Download : Download high-res image (723KB)Download : Download full-size image

Fig. 7. Mass detection results in OMI-H dataset, (a–h) demonstrate detections with high objectness score, (i, j) shows some detections with FPs, and (k, l) shows undetected masses (green: GT box, yellow and red: detection box). The numbers shown in images corresponds to the confidence of being mass.

Table 5. Per-fold confusion matrix results: OMI-H
INbreast (confidence probability 

0.20).
	TP	FP	FN	TN
Fold 1	23	29	2	40
Fold 2	18	14	4	53
Fold 3	27	27	4	40
Fold 4	16	24	3	41
Fold 5	18	51	0	40

Table 6. Mass detection results: OMI-H

INbreast.
Model	Trained on OMI-H	Fine-tuned on INbreast
Malignant	Benign	Malignant	Benign
Sensitivity	0.87	0.55	0.95±0.18	0.71±0.18
Specificity	0.73	0.73	0.70±0.07	0.70±0.08
	0.89	0.67	0.95	0.79
TPR at FPI	0.87 at 0.32	0.55 at 0.32	0.92±0.08 at 0.32	0.71±0.18 at 0.32
0.99±0.03 at 1.17	0.85±0.08 at 1.0
5.4. Qualitative results

In Fig.7, examples of mass detection results are visualised on the mammograms in the OMI-H datasets. Several prediction results are shown: the top two rows show mammograms with precisely predicted masses; ground truth annotations are displayed in green, and the predicted boxes with their confidence scores are in yellow. In Fig.7(i, j) FP detections are shown (red) along with TP, Fig.7(k, l) which show undetected masses. Several mass detection results in the OMI-G and INbreast datasets are visualised in Fig.8 and Fig.9 respectively. Figs. 8(a–h) and 9(a–d) show a single mass detection result at the precise position with a high confidence score in each mammogram, and Fig.9(e, h) shows the detection of several masses in the same mammogram. Some undetected masses are shown in Figs. 8(i) and 9(k, l).

    Download : Download high-res image (522KB)Download : Download full-size image

Fig. 8. Mass detection results in OMI-G dataset, (a–h) shows detections with high confidence score, (i–k) shows detections with FPs, and (l) shows undetected mass (green: GT box, yellow and red: detection box). The numbers shown in images corresponds to the confidence of being mass.

    Download : Download high-res image (619KB)Download : Download full-size image

Fig. 9. Mass detection results on INbreast dataset, (a–d,g) shows detections with high confidence score, (e, h) show multiple detections in the same mammogram, (f, i, j) shows detections with FPs, and (k, l) shows undetected masses (green: GT box, yellow and red: detection box). The numbers shown in images corresponds to the confidence of being mass.
6. Discussions

In this paper, it has been shown that the Faster R-CNN model, pre-trained on an entirely different dataset of natural images, can be fine-tuned to efficiently detect masses in whole mammograms. It has also been shown that enhanced performance can be obtained when the Faster R-CNN model is trained on a large database of mammograms (OMI-H), and then fine-tuned using the mammograms in smaller databases (OMI-G and INbreast).

In the OMI-H dataset, analysis of the undetected masses was also performed based on the conspicuity. It was found that 8.7% of the total obvious masses were not detected (51 out of 601). Moreover, 5 out of 20 very subtle masses (25%), 26 out of 139 subtle masses (18.7%), and 3 out of 4 occult masses (75%) remained undetected using the proposed framework. The mass detection was also analysed based on the malignancy, i.e.malignant (BI-RADS 
{4,5,6}) and benign (BIRADS 

{2,3}). It was found that the detection framework was able to detect malignant masses with a sensitivity of 0.86 at 1.2 FPI and 0.73 at 0.30 FPI, while for benign masses, the model resulted in a sensitivity of 0.81 at 0.72 FPI and 0.65 at 0.30 FPI.

In terms of transfer learning from different scanners, the images in the OMI-G dataset had low contrast compared to the OMI-H dataset. Thus, normalisation of the images in these datasets was performed and the results compared in Table4. Compared to the results obtained for the original mammograms in the OMI-G dataset (
), higher performance was obtained for the normalised images () (

). This can be justified as the Faster R-CNN model was pre-trained on the OMI-H images, which are similar in contrast to the normalised images in the OMI-G dataset. This contrast enhancement benefits the fine-tuning process, and as expected, the system detected approximately 83% of masses in the OMI-G dataset with 0.43 FPIs, and with 1.7 FPIs, more than 90% of masses are detected.

In principle, it would be feasible to mix the two datasets (OMI-H and OMI-G) for training even though sensor sizes (field of view and pixel sizes) have some differences. This can be done in order to increase the training size (especially for OMI-G), given that the input images are converted to the same pixel size (i.e.200

). Although we performed this experiment, testing results for both OMI-H and OMI-G were worse compared to training with the same vendor dataset. This could be explained by the fact that, although performing an intensity normalisation step, images from different vendors still show differences in dynamic range, local contrast, and signal to noise ratio, amongst others, mainly due to their imaging characteristics and post-processing.

The availability of a large dataset to train the Faster R-CNN model has been one of the limitations of the framework proposed in the current work. This is evident from comparing the mass detection performance on the OMI-G dataset using the two approaches: (i) Faster R-CNN trained directly on OMI-G (
), and (ii) Faster R-CNN firstly trained on OMI-H, and then fine-tuned on OMI-G () (). To the best of our knowledge, this is the first study presenting the results on OMI-DB. Although a comparison with any other works could not be established for these datasets, a comparison to other works using large mammography dataset was established. For instance, Kooi etal.[14] obtained an  of 0.93 on an internal database of 45,000 FFDMs; Morrel etal.[23] obtained an  of 0.87 on 13,000 images from Group Health. In this work, we obtained an 

of 0.90 for the malignant masses in the OMI-H dataset.

The benefits of transfer learning were evaluated by fine-tuning the OMI-DB trained model to detect masses in a public dataset (INbreast) and comparisons were done with state-of-the-art methods (Table7). It is evident from the results that the performance of the proposed framework is comparable or higher than existing methods.

The mass detection model of Akselrod-Ballinetal.[45] obtained a TPR of 0.93 at 0.56 FPI on a subset of the INbreast dataset (100 mass images), compared to a TPR of 0.87 at 0.3 FPI obtained in this work (410 images). Although, the detection performance of the proposed framework is slightly lower, there is a difference in the number of images used and thus a one-to-one comparison is difficult to establish. Dhungel etal.[8] proposed a framework consisting of a cascade of deep learning methods. This was aimed at reducing the false positive detections to subsequently improve the precision of the bounding box predictions, obtaining TPR of 0.90
0.02 at 1.3 FPI for the INbreast dataset. In this paper, we obtained a TPR of 0.95

0.03 at 1.14 FPI using a much simpler object detection framework.

Ribli etal.[20] used the Faster R-CNN model with VGG16 and trained the model on the INbreast dataset consisting not only the masses, but also the calcifications. They evaluated the performance on malignant masses and calcifications to obtain a TPR of 0.90 at 0.3FPI. In this paper, we obtained comparatively higher detection results with a TPR of 0.92 at 0.3 FPI. However, a direct comparison is difficult to establish because of differences in the selection of mammograms with malignant masses.

Jung etal.[21] used a one-stage object detection model (RetinaNet) and trained it using an in-house dataset containing malignant masses along with INbreast dataset. It has been shown in previous works[22], that compared to Faster R-CNN, higher performance has been achieved by using RetinaNet for detecting objects in natural images. However, we show that higher performance is achieved for detecting masses in mammograms using the Faster R-CNN model. This can be linked to the fact that a much larger dataset (10 times) of FFDMs, containing malignant masses, has been used to train the Faster R-CNN model compared to 222 FFDMs used for training the RetinaNet model in[21].

Al-masni etal.[9] used the YOLO object detection model to propose a CAD system to handle the detection and classification simultaneously. The authors showed enhanced accuracies when using an augmented DDSM database created by rotating each mammogram multiple times. In other work, Al-antari etal.[10] proposed an automated framework to detect, segment and classify masses in a single framework. As per our understanding, the training and testing sets in both the works are non-exclusive, and thus the one-to-one comparison with the results in this study cannot be established.

In one of our previous work[28], traditional CNN-based methods were employed to obtain patch level predictions which were used to detect masses in whole mammograms by reconstructing the whole image (from patches) using sliding window approach. The framework obtained similar (or better) results than state-of-the-art methods on the INbreast dataset (TPR of 0.98±0.02 at 1.67 FPI). However, the high computational cost associated with the image reconstruction step was a limiting factor for clinical use.

The inference time per mammogram of some of the methods are shown in Table7. Although, the computational environment used for different methods could be different, this gives an overall idea regarding the applicability of the methodology in real-time. The inference time per image of the proposed Faster-RCNN method is 4 s per image using a GPU with 12GB memory, which is slightly higher compared to single-stage methods like YOLO, RetinaNet, etc. (Table7).

In this work, the results are shown in mammograms obtained from three different scanners, i.e.Hologic (OMI-H), GE (OMI-G) and Siemens (INbreast), demonstrating the adaptability of the presented framework to detect masses in the images obtained using different mammography acquisition systems. Furthermore, in the OMI-G dataset, a sensitivity of 0.75 is achieved with a specificity of 0.90, and a TPR of 0.75 at 0.20 FPI. For malignant masses in the public dataset INbreast, a sensitivity of 0.87 is achieved with a specificity of 0.90, and TPR of 0.87 at 0.1 FPI. These results suggest the proposed framework is able to detect masses with a reduced number of false positives and false negatives, demonstrating its potential use as a part of a CAD system to aid radiologists in the detection of breast cancers. As noted previously, the focus of this study is currently on mass detection, so images containing calcifications were not considered for the analysis. This will be investigated further in future studies.

7. Conclusions and future work

In this work, the implementation of a Faster R-CNN model for detecting masses in a large-scale dataset of breast mammograms was presented. It was shown that the Faster R-CNN model, pre-trained on an entirely different dataset of natural images, could be adapted to efficiently detect masses in whole mammograms. It was also shown that enhanced performance could be obtained when the Faster R-CNN model was trained on a large database of mammograms and fine-tuned to adapt it for other mammograms in smaller databases (OMI-G and INbreast). Compared to other works in the literature, the proposed mass detection framework showed improved performance in terms of higher sensitivity and lower FPI.

The proposed framework has the potential to be used within the clinical environment as it takes the whole mammogram as the input and outputs the suspected masses within the mammogram. Moreover, the presented framework has been tested to detect masses in two different small mammography datasets obtained using different mammography acquisition systems. This demonstrates the potential of the detection framework for analysing mammograms from different systems, which is a particular requirement for successful deployment in different clinical environments.

In terms of future work, we intend to extend this to classify masses into benign and malignant. Moreover, it would be interesting to perform an observer study involving experienced radiologists (at least 2–3) and comparing their standalone performance results against the mass detection tool presented here. Additionally, an investigation would be performed to adapt the presented method to detect masses in 3D volumes such as Automated Breast Ultrasound and Digital Breast Tomosynthesis[50], which are currently being adopted in clinical practice.
Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.